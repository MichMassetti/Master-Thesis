%% LaTeX2e class for student theses
%% sections/evaluation.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute of Information Security and Dependability (KASTEL)
%%
%% Template by
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Adaption by
%% Annika Vielsack
%% vielsack@kit.edu
%%
%% Version 1.0, 2021-07-03

\chapter{Evaluation}
\label{ch:Evaluation}
This chapter covers the outcomes of the tools, basically, what we obtained from our experiments. 

Firstly, the individual outcomes per tool are presented, providing details about the running of those, 
problems that occured during the installation and usage. 
Then, a comperison of tools is provided based on different aspects. 
Comperions on subsets of those are used for focusing on a spefic class of those.




\section{Individual Outcomes per Tool}

This section deals with the eventual problems during the installation and running phase , 
moreover, the tools outcomes having as objective a specific attack are collected. 
Per each tool, a table has three parameters; the first one is the number of written lines of code considering the specifications. 
The part of the code which were copied from the target smart contracts were discarded during the counting. 
The second field deals with the execution time of the tool and the last one specifies if the vulnerability was effectively detected.

The code of the exploits was modified to keep the vulnerable aspects, reproducing their logic, but the irrelevant parts were discarded.

We kept the same simplified version of the smart contracts for all the tools.

\paragraph{Manticore} The provided guide on Github (\cite{ManticoreGitHub}) gives a detailed guide for Manticore installation. 
Since it is written in python, we used a virtual environment and we counted around nine libraries for dependences. 
The installation involved basically one command, since it was managed by "pip", python packet manager.
We adopted its default running mode for the attacks involving reentrancy and "Manticore-verifier" running mode for the others. 

The table \autoref*{tab:ManticoreTable} shows the outcames of the analyses per attack; the symbol "--" means that in that case we did not use the 
"Manticore-verifier" running mode, but the default one, which it does not need a specification file.

\begin{center}
\begin{table*}
    \caption{Manticore results}
        \label{tab:ManticoreTable}
        \begin{tabular}{cccc}
        \toprule
            Attacks & Lines of Code & Execution Time (seconds) & Foud, Not Foud\\
            \midrule
            Aku & 6 & 235  & Not Found \\ 
            Cover & 5 & 245 & Not Found \\ 
            BZX  & 4 & 228 & Found \\ 
            Spartan & 3 & 239 & Found \\ 
            Uranium  & 3 & 250 & Not Found \\ 
            XSURGE & -- & 208 & Found \\ 
            BurgerSwap  &  -- & 205 & Found\\ 
            DirtyDogs & -- & 203 & Found \\
        \bottomrule
    \end{tabular}
\end{table*}
\end{center}

\paragraph{SmarTest} SmarTest is built on top of VeriSmart tool, so it can be seen like a plug in of these one. Indeed, we run SmarTest as an option of VeriSmart, 
as the Github guide explains (\cite{SmarTestGitHub}). The tool is built with OCalm, a program language, so we used for the installation "opam", which is a source-based package manager for it.
An important dependency is Z3, its satisfiability modulo theories (SMT) solver. Solc, the compiler for solidity, is required.

Since it has no the dector for reentrancy, as the table \autoref{tab:SmarTestTable} shows, the attacks involving it were discarded. 
We adopted just the "assertion" running mode, for obtating valuable results. 
We fixed a running time threshold of 320 seconds.

\begin{center}
    \begin{table*}
        \caption{SmarTest results}
            \label{tab:SmarTestTable}
            \begin{tabular}{cccl}
            \toprule
                Attacks & Lines of Code & Execution Time (seconds) & Foud, Not Foud\\
                \midrule
                Aku & 3 & 320 & Found \\ 
                Cover & 2 & 310& Not Found\\ 
                BZX  & 4 & 320 & Found\\ 
                Spartan & 2 & 320& Not Found\\ 
                Uranium  & 4 & 320 & Found \\ 
                XSURGE &  -- & -- & Not Found \\  
                BurgerSwap &  -- & -- & Not Found\\ 
                DirtyDogs &  -- & -- & Not Found \\
            \bottomrule
            \end{tabular}
        \end{table*}
\end{center}


\paragraph{Celestial} 
Celestial is a tool which encompasses two main steps: the translation of the target smart contract in F* and then the running of the formal verification engine. 
This is the tool that required the most amount of time for installation and usage. 
It did not cover reentrancy attacks, so those exploits were discarded, moreover, it did not cover the keywords "storage" and "memory", 
so the "Cover Protocol" exploit was discarded as well. \autoref{tab:CelestialTable} shows the tool could detect the other four vulnerabilities.
During the running, we used multiple versions of F* and in some cases (as with the "Uranium" exploit) the conversion was not correct so we adjusted the F* code to let it work. 


\begin{center}
    \begin{table*}
        \caption{Celestial results}
            \label{tab:CelestialTable}
                \begin{tabular}{cccl}
                \toprule
                    Attacks & Lines of Code & Execution Time (seconds) & Foud, Not Foud\\
                    \midrule
                    Aku & 22 & 4 & Found\\ 
                    Cover & --  & -- & Not Found \\ 
                    BZX & 15 & 3 & Found\\ 
                    Spartan & 29 &  5 & Found \\ 
                    Uranium & 18 &  5 & Found \\ 
                    XSURGE &  -- & -- & Not Found \\  
                    BurgerSwap &  -- & -- & Not Found\\ 
                    DirtyDogs &  -- & -- & Not Found \\
                \bottomrule
                \end{tabular}
    \end{table*}
        
\end{center}
\paragraph{Echidna} Echidna is a fuzzer for smart contracts. The tool did have any problem during the installation and worked fluentily. 
It allows to write the properties in form of function or as statement of "assertion". Both solution were applied depeing on the case.
For "BZX" and "Spartan" exploits assertion mode was, the other one for the other cases. 
\autoref{tab:EchidnaTable} states that the tool could detect all the vulnerabilities, 
but the ones involving external calls, since it discards those.


\begin{center}
    \begin{table*}    
        \caption{Echidna results}
        \label{tab:EchidnaTable}
        \begin{tabular}{cccl}
        \toprule
            Attacks & Lines of Code & Execution Time (seconds) & Foud, Not Foud\\
            \midrule
            Aku & 7 & 22 & Found\\ 
            Cover & 5 & 7 & Found\\ 
            BZX & 3 & 33 & Found \\ 
            Spartan & 3 & 17 & Found  \\ 
            Uranium & 3 & 24 & Found \\ 
            XSURGE & -- & -- & Not Found \\  
            BurgerSwap &  -- & -- & Not Found \\ 
            DirtyDogs &  -- & -- & Not Found \\
        \bottomrule
        \end{tabular}
    \end{table*}
\end{center}

\paragraph{Certora} Since Certora is not an open source tool, the provided documentation is limited. 
Their GitHub (\cite{CertoraGitHub}) has a repository dedicated to tutorials for understanding how to write down the properties. 
\autoref{tab:CertoraTable} states that the tool could detect all the vulnerabilities, 
but the ones involving external calls, since it discards those.

\begin{center}
    \begin{table*}
        \caption{Certora results; the time is provided by the sas application}
            \label{tab:CertoraTable}
            \begin{tabular}{cccl}
            \toprule
                Attacks & Lines of Code & Execution Time (seconds) & Foud, Not Foud\\
                \midrule
                Aku & 52 & 14 & Found \\ 
                Cover & 31 & 21 & Found\\ 
                BZX & 25  & 18 & Found\\ 
                Spartan & 20  & 25 & Found\\ 
                Uranium & 42 & 27  & Found\\ 
                XSURGE &  -- & -- & Found\\  
                BurgerSwap &  -- & --& Found\\ 
                DirtyDogs &  -- & -- & Found\\
            \bottomrule
            \end{tabular}
    \end{table*}
\end{center}

\paragraph{SolcVerify} SolcVerify deals with formal verification for smart contracts. 
Its annotation language does not require a great amount of lines of codes. 
For working properly, as Celestial, all the involved functions requires specifications. 
Installation problems occured during the right configuration .NET and the selection of the right version of the external dependences.
Moreover, the tool had issues for finding the correct path of those in the system.

\autoref{tab:SolcVerifyTable} shows the tool could detect all the vulnerabilities.

\begin{center}
    \begin{table*}
        \caption{SolcVerify results}
                \label{tab:SolcVerifyTable}
                \begin{tabular}{cccl}
                \toprule
                    Attacks & Lines of Code & Execution Time (seconds) & Foud, Not Foud (seconds)\\
                    \midrule
                    Aku & 9 & 4 & Found \\ 
                    Cover & 13  & 5 & Found \\ 
                    BZX & 17  & 9  & Found \\ 
                    Spartan & 25 &  17 & Found \\ 
                    Uranium  &  23 & 9 & Found \\ 
                    XSURGE & 20 & 10 & Found \\  
                    BurgerSwap & 11 & 10  & Found \\ 
                    DirtyDogs &  30 &  14& Found \\
                \bottomrule
        \end{tabular}
    \end{table*}
    
\end{center}

\paragraph{Slither and Mythril} Slither and Mythil did have any problem during the installation phase and run properly per each attack. 
They could detect all the reentrancy issues, but none of the others.

\section{General Comparison}
%% presenting the outcomes of single analyses, so description of the result 
%% this subsection contains objective data, in the next one I add even my point of view

The first step was the collection of all the data from the analyses with the vulnerable smart contracts as objective. 
In this part, we provide a technical comperison of the tools. 


\subsection{Installation} 
\autoref{tab:Installation} collects the info about the installation detalais per each tool. 

The running mode involves the way a tool can be run, a tool is classified as "multiple running mode", if its grammar effectivly changes between different modes. For example echidna, it can be run in 
test mode and in assertion one, in the first case it requires functions with boolean formulas, in the second case the solidity keyword "assertion" in the code.

The external dependences encompass the exteranal elements, so we do not count the amount of libraries of the same language. 
All the tools require Solc, the solidity compiler. 
Most of the tools did not have any problem during the installation phase. Celestial and SolcVerify installations had issues.  
The first one needs F* for running and the selection of its right version is
based on the tool and the system environment, consequently, it turned like a challenge. 
On the other hand, the second one involves the usage of Boogie, an intermediate verification language, which requires .NET, (\cite{NET}), 
an open source, cross-platform for building many kinds of applications mantained by Microsoft. 
The configuration of the correct version of .NET and the other external dependences was based on the computer enviroment and the tool. 

Certora operates in Service as a Service (SaaS) mode, so it can be classified as the most compatible tool, but it requires to be connect to internet.
SmartTest can be run in two modes, but it was not necessary, since the running mode without specifications could not provide any consistent results.


\begin{center}
    \begin{table*}
        \caption{Installation and running mode}
        \label{tab:Installation}
        \begin{tabular}{ccccc}
        \toprule
            Tools  &  Running modes & Extenral Dependences & OS \\
            \midrule
            Manticore & 2 & 2 & Linux, OS X\\
            SmartTest & 2 & 3 & Linux, OS X, Windows \\
            Celestial & 1 & 3 & Linux, OS X\\
            Echidna & 2 & 2 & Linux, OS X, Windows\\
            Certora & 1 & -- & -- \\ 
            SolcVerify & 1 & 4  &  Linux, OS X \\
            Mythril  & 1 & 2  &  Linux, OS X \\ 
            Slither & 1 & 2 & Linux, OS X, Windows \\   
        \bottomrule
        \end{tabular}
    \end{table*}
\end{center}

\subsection{Outcomes} 
In this subsection, the outocomes of the tools are compared. 
\autoref{tab:Results} has the name of tools as rows and the real-world exploits as columns.
It shows which tool was able to scan the specified vulnerability involved in the attacks. 
The caption states the symbols used for the evaluation:
\begin{itemize}
    \item \checkmark states that the vulnerability was scanned;
    \item \xmark means the tool was no able to detect it;
    \item -- stands for "discarded", tha attack was not considered for architectural reason of the tool.
\end{itemize}


We can state that the tools with specifications could not detect reentrancy in most of the cases. 
SolcVerify is the only one which could provide a result in all cases, moreover, it was the only tool which could detect correctly all the vulnerabilities we provided.

Celestial is the tool with more discarded cases. 
The tools without specification could not detect any of the attacks, but the ones involving external calls.  

The tools without specifications gave warning in most cases, which can be used as a hint for scanning the vulnerabilities. 
Echidna and Certora had a similar behaviour, they could not detect the attacks regarding reentrancy.


\begin{center}
    \begin{table*}   
        \caption{Analyses Outocomes per Attack:    
        \checkmark: Found vulenrablity, \xmark: Not found vulnerability, --: Discarded }
        \label{tab:Results}
        \begin{tabular}{ccccccccc}
        \toprule
        Tools  & Aku & Cover & BZX & Spartan & Uranium & XSURGE &  BurgerSwap & DirtyDogs\\
        \midrule
        Manticore & \xmark & \xmark & \checkmark & \checkmark & \xmark & \checkmark & \checkmark & \checkmark\\
        SmartTest & \checkmark &   \xmark & \checkmark  & \xmark &\checkmark  & -- & -- & --  \\
        Celestial & \checkmark & -- & \checkmark & \checkmark & \checkmark & -- & -- & --  \\
        Echidna  & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & -- & -- & -- \\
        Certora & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & -- & -- & -- \\ 
        SolcVerify & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark  & \checkmark \\
        Slither & \xmark &\xmark  &\xmark & \xmark & \xmark & \checkmark & \checkmark & \checkmark \\ 
        Mythril  & \xmark & \xmark & \xmark &\xmark & \xmark & \checkmark & \checkmark & \checkmark\\
        \bottomrule
        \end{tabular}
    \end{table*}
\end{center}

\autoref{tab:Outcomes} provides high level detailes about the analyses per tool. 
"Constructive output" is referred to the type of output of the tool. 
We have three types:
\begin{enumerate}
    \item list of functions: the sequence of operations to execute for forcing the vulnerability is expressed;
    \item list of unproved tests: the output provides only the name of the failed test;
    \item warnings: the tool displays just warnings.
\end{enumerate}

SolcVerify and Celestial, within the tools allowing custom analyses, are the only ones which do not provide any additional information. 
Manticore and Mythril display, when it is possible, the list of functions, but just a warnings of possible risk, in the case of reentrancy. 

Reagading the speed of the tools, Celestial tets times are assumed with zero delay between the generation of the F* code and the running of the F* module. 
The time spent for running by command line the two different operations could be avoided by automating the process.
The slowest tools are the ones involving symbolic execution. Mythtil needs a similar amount of time as Manticore and SmartTest, even if it does not allow custom analyses. 
Slither is fastest one, but specifications are not provided per test and no additional information are displayed about the scanned vulnerability.
\begin{center}
    \begin{table*}
        \footnotesize
        \caption{Analyses Outcomes: 
        LoF: List of functions, LoU: List of unproved tests, W: Warnings}
        \label{tab:Outcomes}
        \begin{tabular}{ccccc}
        \toprule
        Tools  & Constructive output &  Avg lines of code for test & Avg time (in seconds) \\
        \midrule
            Manticore & LoF, W  & 4  &  239,5 \\
            SmartTest & LoF, W & 2,5 &  318  \\
            Celestial & LoU & 21  &  4  \\
            Echidna & LoF  & 4  & 20,5 \\
            Certora & LoF   & 34 &  21  \\ 
            SolcVerify & LoU  &  18,5 &  10  \\
            Mythril & LoF, W  & --  &  221  \\ 
            Slither& W & --  &  3,5  \\ 
        \bottomrule
        \end{tabular}
    \end{table*}
\end{center}




\chapter{Discussion}
\label{ch:Discussion}

This chapter deals with the discussion of the obtained results. 
We provide a comparison between the tools about their behaviour, including our point of view on user experience. 
\section{Tools with Specifications}
%% General Overview of the strategies 
%% Comperison within the strategies 
%% Personal ideas on their usage, how was the language and so on 
The considered tools cover three main strategies for security analysis: fuzzing, symbolic execution and formal verification. 

The following paragraphes deal with comparison within this subgroup we want to stess for having a deepen view of the comparison and a last sub chapter for a general overview.

\paragraph{Formal verification} 
Formal verification is a very powerful security approach, intending to prove or unproved the given specification. 
This perfectly fits with our research goals, such as the detection of bugs or vulnerabilities in our real-world cases. 

We involved three different tools, implementing this approach, for our purpose. Regarding the results, it is clear that these were executed as fast as or even faster than the other ones. 
Certora is the only one which provides a complete list of functions for breaking the rules, rather than just a warning. 
On the other hand, SolcVerify could detect the vulnerabilities involving external call functions, indeed reentrancy. A powerful aspects of this tool is its possibility to express 
loop invariants, the other ones do not allow it.
Considering the grammar for expressing the specifications, SolcVerify is the one which needs the least amount of lines of code, indeed it involves a notification language; 
I found it very intuitive and fast to write down the specifications.

Celestial architecture encompasses two steps: the translation from celestial file to f* and then its verification. The python script converts the ".cel" in f*, used for the proof or unproof.
The provided file included the smart contract's source code plus the expressed specification. These are statements placed at the beginning of a function, otherwise, it is possible to create a sort of function, 
containing boolean formula, which is called by different specifications function with different parameters, it is useful for expressing the same specification for different purpose.  I consider it the one with more limitations, regarding solidity grammar and reentrancy, 
because it could not detect the reentrancy vunlenaribilities and the Cover protocol attack, because the keywords "storage" and "memory" are rejected. 

Certora is the only tool which is not open-source, for our purpose we adopted its free version.
Its specification language is described by its developers' group as "rule-based". It differs from the other two tools under this aspect, because this way gives more elasticity to the user and defines more specific cases.
The rule is composed of some function calls and it concludes with an assertion or more. The user is allowed to test a specific case, using "require" and the possibility to set up a proper environment. 
The preconditions, in this case, are expressed using the Solidity keyword "require" in the rule.

One of its strengths is the possibility to define the specifications we want to prove, without necessarily defining all the specifics for the rest of the functions.
On the other hand,  with Celestial and SolcVerify, we provided the specifications for all the functions for letting the tools work properly. 
Those could not prove the given properties, without the specifications for all the code.

\paragraph{Different stategies, similar grammar}
We considered two tools with similar grammar but implemented different analysis approaches: Manticore and Echidna. 
In both of the cases, we provided functions containing a boolean formula, which the tools try to break. From the results, 
we noticed that Echidna run faster and it worked for all the cases, but Manticore could cover the reentrancy vulnerabilities thanks to its changing architecture.

We encountered a common aspect between the grammar definition of the specification between the two tools Echidna and SmarTest regarding their "assertion" mode. 
Both of those require the user to write assertions and then these try to verify it or return the list of 
functions for breaking the rule. From our results, it is clear Echidna could obtain higher number of positive outomces and in less time rather than SmartTest.



\section{Customized and Non-specific Analyses} 
The objectives of our analyses were smart contracts involved in real-world exploits. The attackers exploited a specific bug or lack of security in the logic of programs. 

The specifications allowed the user to express the requirements of the program. These provide a customized analysis and 
its accuracy is demanded by the developer as well. 
Our work involved smart contracts with well-known vulnerabilities, but the definition of the properties is an indispensable and complex step.
The tools without specification implement vulnerabilities detectors, which are stated in their paper. They have well-defined limitations, but strengths as well. These detect preconfigured vulnerabilities, so a warning can be run even if we expect that indeed the number of false negatives can be relevant.
An example is Slither, which for every comparison of block time stamp give a warning. These warnings 

Our results stress the impossibility of the tools without specification for the detection of specific vulnerabilities, but they could correctly detect the reentrancy cases. 
On the other hand, the ones with specifications had problems in the reentrancy detection. The limitations of some of these involve the external calls. 
SolcVerify was the only tool which could provide the possibility of reentrancy detection. Echidna, as Certora, developers teams specified the tools  can detect the reentrancy, in the case an attack is provided, 
but we think this approach can be useful for checking a possible attack rather than detection of vulnerabilities.

Manticore could bridge this gap by adopting two different running modes, so the user, knowing the limitation of each way of analyses, can combine those for obtaining a valuable result.


\section{Effective Analysis}
%% Use tools in combination
%% General thoughts on the tools
%%basically it covers the chapter of weaknesses and strenhgs 
In our work, we took into consideration the selected tools individually. 

We run those per time focusing on the results of each one, and providing a comparison between those.

During an audit or a security report, a tester runs multiple of those for discovering vulnerabilities and bugs. 
A better way to fulfil this goal is using a combination of those. 
The tools without specification have, in our experience, an easier installation and usage, 
due to a reduced amount of external dependences and writing down specification is not required.
Those can detect well know vulnerabilities and cover a predefined set of those. 

Some of the tools with specifications we dealt with had some limitations regarding the external calls: just SolcVerify covered this set of vulnerabilities. 
For covering this limitation, a combination of tools would be a solution. 

We should consider a tool without specification, Slither, and one with, Echidna. 
This combination has an effective result in terms of the speed of the analyses and amount of vulnerabilities covered. 
Slither has the role of detecting basic issues and reentrancy, on the other hand, Echidna can be used for the detection of a vulnerable implementation of the logic of the program. 
Since the grammar of this is similar to Manticore, an efficient solution would be to implement the same specification using it, which implements a different logic for scanning. 

Formal verification resulted powerful for scanning possible problems, but SolcVerify and Celestial require to 
write down the specification for all the contracts for obtaining a consistent result. 
Certora has the strength of having implemented libraries which are mostly used in real-world cases (as OpenZeppelin ones,Aeve protocol). 
A facilitating aspect of this tool is the possibility to write down the specifications on just the properties we want to check and 
the possibility to code those in terms of function. It allows for the definition of specific preconditions, adding conditions to the environment. 
Another strenght of this tool is the possibility of using it as SaaS, consequently, the computational effort is demanded to another computer.

SolcVerify is the tool which obtained the best results in term of discovering vulnerabilities. One of the drawback we found is the è

We suggest a combination of Echidna and Certora for covering the part of bugs in the logic and possible attacks, 
plus Slither for verifying the absence of possible reentrancy. 
We selected this combination based on the facility we had during the write down of the specifications and installation of those.

