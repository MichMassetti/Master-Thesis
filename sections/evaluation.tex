%% LaTeX2e class for student theses
%% sections/evaluation.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute of Information Security and Dependability (KASTEL)
%%
%% Template by
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Adaption by
%% Annika Vielsack
%% vielsack@kit.edu
%%
%% Version 1.0, 2021-07-03

\chapter{Results and Evaluation}
\label{ch:Evaluation}
This chapter presents the results of the analyses. It is addressed to show the obtained data from the tests.

It deals with the individual outcomes per tool, providing details about the running phase and installation.
Eventual problems, during those phases, are described per each tool.

Then, a general comparison gives a high-level view of the obtained results.
This section aims to define which tool had better behavior in terms of operability, speed and correctness of the analysis. 

\section {Experimental Setup}
The analysis tools were installed and used on a laptop using Ubuntu 20 as the operating system.
The first step for launching the analysis was the selection of smart contracts.
Their vulnerable sections were considered and the safe parts were discarded, while preserving the foundamental logic.
The changes applied to the smart contracts have the aim to simplify those and make the analysis faster since the tools have less amount of code to scan.

The smart contracts were adopted to be tested by all the tools.
The aim was to obtain results from tools having the same targets.

In Solidity the standard for implementing the token is ERC20.
Every token inherits the standard interface IERC20.
Their implementations are substituted with arrays, which store the information of the balance of each user.

All smart contracts are adapted to be compiled with Solidity version 7.
Echidna, Celestial and SmarTest had problems with version 8.
The eighth version solves the problem regarding algebraic issues:
transactions automatically revert in case of any problem during an algebraic operation.
In our case, all the smart contracts implemented the "SafeMath", which provides the same functionality check during algebraic operations.


The constructors can implement different initialization based on the requirements of the tools.
Following, some examples on how smart contracts are adapted for running the tests.

The auction of the Aku protocol (\autoref{sec:Exploits:AkuDreams}) functionalities are reduced.
The user can only buy bids and call the function for processing the refunds.
Moreover, the owner can call the function \texttt{claimProjectFunds} for claim the funds of the project.

Bzx exploit (\autoref*{sec:Exploits:bZX}) involves a vulnerability in the implementation of the ERC20 standard token interface.
The functions regarding the transfer of tokens and the estimation of the balance are not changed,
but the other functionalities, such as mint and burn are not considered.

In the Cover protocol exploit(\autoref{sec:CoverProtocol:Exploit}), the base struct of the pool and its vulnerable function \texttt{deposit} are kept.
Other functionalities are discarded.

Spartan Protocol (\autoref*{sec:Exploits:Spartan}) is written over one smart contract; imports are avoided.
The balances of the underlying assets are not estimated, calling the function \texttt{balanceOf} of their smart contracts.
The balances of the users are stored in an array.
The variables \texttt{tokenAmount} and \texttt{baseAmount} have the same roles.

The Uranium exploit (\autoref{sec:Exploits:Uranium}) simplified version involves the three main functions of the attacks
\texttt{deposit}, \texttt{withdraw} and \texttt{emergencyWtihdraw}.
The struct of the user, which stores the number of tokens and the rewards, is not modified.
In this case, the rewards are not sent to any user, but an array stores the amount of the rewards per user.


Considering an example of reentrancy, DirtyDogs smart contract(\autoref{sec:CoverProtocol:Exploit}) inherits the same version of the ERC777 NFT standard.
It keeps the vulnerability, but the potentiality of the contract is reduced to the \texttt{ClaimDogs} function.
The constructor initializes the array which stores the number of tickets per user.

\section{Individual Outcomes per Tool}
This section deals with the eventual problems during the installation and running phase.
Moreover, the tools outcomes having as objective a specific attack are collected.
Per each tool, a table has three parameters; the first one is the number of written lines of code considering the specifications.
The part of the code which were copied from the target smart contracts were discarded during the counting.
The second field deals with the execution time of the tool and the last one specifies if the vulnerability was effectively detected.

Per each tool with specifications, some examples of specifications are provided.

\subsection*{Manticore} The provided guide on Github (\cite{ManticoreGitHub}) gives a detailed guide for Manticore installation.
Since it is written in python, we used a virtual environment and we counted around nine libraries for dependences.
The installation involved basically one command, since it was managed by "pip", a python packet manager.

The table \autoref*{tab:ManticoreTable} shows the outcomes of the analyses per attack; the symbol "--" means that in that case we did not use the
"Manticore-verifier" running mode, but the default one, which it does not need a specification file.

\begin{center}
\begin{table*}
    \caption{Manticore results}
        \label{tab:ManticoreTable}
        \begin{tabular}{cccc}
        \toprule
            Attacks & Lines of Code & Execution Time (seconds) & Foud, Not Foud\\
            \midrule
            Aku & 6 & 235  & Not Found \\ 
            Cover & 5 & 245 & Not Found \\ 
            BZX  & 4 & 228 & Found \\ 
            Spartan & 3 & 239 & Found \\ 
            Uranium  & 3 & 250 & Not Found \\ 
            XSURGE & -- & 208 & Found \\ 
            BurgerSwap  &  -- & 205 & Found\\ 
            DirtyDogs & -- & 203 & Found \\
        \bottomrule
    \end{tabular}
\end{table*}
\end{center}

Its default running mode activates the automatic modules for the detection of reentrancy. So its mode without specifications was used for attacks involving external calls.
"Manticore-verifier" running mode, the one with specifications, was adopted for the other exploits. 

\autoref*{lst:ManticoreTest} is an exmple of "Manticore-verifier". 
The objective of the analysis is Cover Protocol (\autoref*{sec:Exploits:CoverProtocol}). 
Since the tool does not allow the any functionality for taking the old value of a variable for the test, an array of support is introduced \texttt{minersRewardEff}. 
It stores the rewards per user estimated at the end of the function when the pool is updated.

\begin{lstlisting} [language=Solidity, caption={Manticore Specifications}, label={lst:ManticoreTest}]
    
    function crytic_test_check()public view returns (bool){
        return minersReward[msg.sender]== minersRewardEff[msg.sender];

    }
\end{lstlisting} 

\subsection*{SmarTest} SmarTest is built on top of VeriSmart tool, so it can be seen like a plug in of these one. Indeed, we run SmarTest as an option of VeriSmart, 
as the Github guide explains (\cite{SmarTestGitHub}). The tool is built with OCalm, a program language, so we used for the installation "opam", which is a source-based package manager for it.
An important dependency is Z3, its satisfiability modulo theories (SMT) solver. Solc, the compiler for solidity, is required.

Since it has no the dector for reentrancy, the table \autoref{tab:SmarTestTable} shows the attacks involving external call were not scanned. 
A running time threshold of 320 seconds was set, because the machine, which ran the test, after that threshold gave bugs or stopped automatically.

\begin{center}
    \begin{table*}
        \caption{SmarTest results}
            \label{tab:SmarTestTable}
            \begin{tabular}{cccl}
            \toprule
                Attacks & Lines of Code & Execution Time (seconds) & Foud, Not Foud\\
                \midrule
                Aku & 3 & 320 & Found \\ 
                Cover & 2 & 310& Not Found\\ 
                BZX  & 4 & 320 & Found\\ 
                Spartan & 2 & 320& Not Found\\ 
                Uranium  & 4 & 320 & Found \\ 
                XSURGE &  -- & -- & Not Found \\  
                BurgerSwap &  -- & -- & Not Found\\ 
                DirtyDogs &  -- & -- & Not Found \\
            \bottomrule
            \end{tabular}
        \end{table*}
\end{center}

The tool can be run with specifications using \texttt{assert} function.
In a running code, if the boolean statement is false, the transaction reverts.
The property to be checked is written down in the \texttt{assert}, as \autoref*{lst:SmarTestTest} shows.
The target is the function \texttt{removeLiquidityForMember} of Spart protocol (\autoref*{sec:Exploits:Spartan}).
The postcondition is expressed at the end of the function.

\begin{lstlisting} [language=Solidity, caption={SmarTest Specifications}, label={lst:SmarTestTest}]
    function removeLiquidityForMember() public returns (uint256 outputBase, uint256 outputToken) {
    ...    
    assert(tokenAmount==balanceToken[address(this)]);
    assert(baseAmount==balanceBase[address(this)]);

    }
\end{lstlisting} 

\subsection*{Celestial} 
Celestial is a tool which encompasses two main steps: the translation of the target smart contract in F* and then the running of the formal verification engine.
This is the tool that required the most amount of time for installation and usage.
It did not cover reentrancy attacks, so those exploits were discarded; moreover, it did not cover the keywords "storage" and "memory",
so the "Cover Protocol" exploit was discarded as well. \autoref{tab:CelestialTable} shows the tool could detect the other four vulnerabilities.
During the running, we used multiple versions of F* and in some cases (as with the "Uranium" exploit) the conversion was not correct, so we adjusted the F* code to let it work. 
\begin{center}
    \begin{table*}
        \caption{Celestial results}
            \label{tab:CelestialTable}
                \begin{tabular}{cccl}
                \toprule
                    Attacks & Lines of Code & Execution Time (seconds) & Foud, Not Foud\\
                    \midrule
                    Aku & 22 & 4 & Found\\ 
                    Cover & --  & -- & Not Found \\ 
                    BZX & 15 & 3 & Found\\ 
                    Spartan & 29 &  5 & Found \\ 
                    Uranium & 18 &  5 & Found \\ 
                    XSURGE &  -- & -- & Not Found \\  
                    BurgerSwap &  -- & -- & Not Found\\ 
                    DirtyDogs &  -- & -- & Not Found \\
                \bottomrule
                \end{tabular}
    \end{table*}
        
\end{center}

\autoref{lst:CelestialTest} presents an example of specification file. 
It is an extract of the celestial file of the BZX protocol (\autoref*{sec:Exploits:bZX}). 
The content of the function is the same and the postcondition is expressed at the beginning of the target function.
The specificatons are written as modifiers of the function.  

\begin{lstlisting} [language=Solidity, caption={Celestial Specifications}, label={lst:CelestialTest}]

    function transferPrivate(address _from, address _to, uint _value)
        public
        post checkTransfer(balances[_from],new(balances)[_from],balances[_to],new(balances)[_to],_value)
    {
        ...
    }
\end{lstlisting} 
\subsection*{Echidna} Echidna is a fuzzer for smart contracts. The tool did have any problems during the installation and worked fluently.
It allows to write the properties in the form of a function or as a statement of "assertion". Both solution were applied depending on the case.
\autoref{tab:EchidnaTable} states that the tool could detect all the vulnerabilities,
but the ones involving external calls, since it discards those.

\begin{center}
    \begin{table*}    
        \caption{Echidna results}
        \label{tab:EchidnaTable}
        \begin{tabular}{cccl}
        \toprule
            Attacks & Lines of Code & Execution Time (seconds) & Foud, Not Foud\\
            \midrule
            Aku & 7 & 22 & Found\\ 
            Cover & 5 & 7 & Found\\ 
            BZX & 3 & 33 & Found \\ 
            Spartan & 3 & 17 & Found  \\ 
            Uranium & 3 & 24 & Found \\ 
            XSURGE & -- & -- & Not Found \\  
            BurgerSwap &  -- & -- & Not Found \\ 
            DirtyDogs &  -- & -- & Not Found \\
        \bottomrule
        \end{tabular}
    \end{table*}
\end{center}

\autoref{lst:EchidnaTest} shows the specification file of the exploit Uranium (\autoref*{sec:Exploits:Uranium}).
Since it is a fuzzer, it generates random semi-random inputs and uses real values, not symbolic ones.
For this reason, it needs an initialization.
This tool does not allow to use of annotations as "old\_value\_of", so the properties are changed to obtain the same result.
The constructor contains real values, fixed arbitrarily.
The function that expresses the specification checks the impossibility of obtaining a high bonus with that deposited amount.
The bonus depends even on the time of staking; consequently, the value in the faction is too high to be achieved in a few transactions.
The specification is triggered when the reward, the \texttt{radsbalance}, are higher is higher than 55 and the amount of token, \texttt{depositAccount},
is higher than the original value of 100.

\begin{lstlisting} [language=Solidity, caption={Echidna Specifications}, label={lst:EchidnaTest}]
    constructor()public {
        ai=0;
        radsTot=10000;
        radsbalance[echidna_caller]=0;
        userInfo[echidna_caller].amountWithBonus=0;
        userInfo[echidna_caller].bonus=0;
        depositAccount[echidna_caller]=100;
        
        // staking pool
        poolInfo=PoolInfo({
            lpSupply: 0,
            accRadsPerShare: 0
            });

    }
    ...
   
    function crytic_test_check()public view returns (bool){
        return radsbalance[echidna_caller]<=55 || depositAccount[echidna_caller]<100 ;
    }
\end{lstlisting} 

\subsection*{Certora} Since Certora is not an open source tool, the provided documentation is limited. 
Their GitHub (\cite{CertoraGitHub}) has a repository dedicated to tutorials for understanding how to write down the properties. 
\autoref{tab:CertoraTable} states that the tool could detect all the vulnerabilities, 
but the ones involving external calls, since it discards those.

\begin{center}
    \begin{table*}
        \caption{Certora results; the time is provided by the sas application}
            \label{tab:CertoraTable}
            \begin{tabular}{cccl}
            \toprule
                Attacks & Lines of Code & Execution Time (seconds) & Foud, Not Foud\\
                \midrule
                Aku & 52 & 14 & Found \\ 
                Cover & 31 & 21 & Found\\ 
                BZX & 25  & 18 & Found\\ 
                Spartan & 20  & 25 & Found\\ 
                Uranium & 42 & 27  & Found\\ 
                XSURGE &  -- & -- & Found\\  
                BurgerSwap &  -- & --& Found\\ 
                DirtyDogs &  -- & -- & Found\\
            \bottomrule
            \end{tabular}
    \end{table*}
\end{center}

As an example of a specification file of Certora, \autoref{lst:CertoraTest} presents the function \texttt{CheckUnderlyingAssets}, called rule, which was not proven by the tool during the analysis of Spart Protocol (\autoref{sec:Exploits:Spartan}).
As the attack, the output presents a possible solution the sending of funds to the underlying assets to modify the estimation of the number of tokens. 
Certora requires that a variable environment is declared and passed to the function. 
The user can set the amount of native tokens to send and the address of the sender.

\begin{lstlisting} [language=Solidity, caption={Echidna Specifications}, label={lst:CertoraTest}]
    rule CheckUnderlyingAssets(address account) {
        env e;
        //e.msg_sender = 
        //e.msg_value = 
        removeLiquidityForMember(e);
    
        assert balanceBaseOfThis() <= getBaseAmount();
    
    }
\end{lstlisting} 

\subsection*{SolcVerify} SolcVerify deals with formal verification for smart contracts.
Its annotation language does not require a great amount of lines of code.
For working properly, as Celestial, all the involved functions require specifications.
Installation problems occurred during the right configuration .NET and the selection of the right version of the external dependences.
Moreover, the tool had issues for finding the correct path for those in the system.

\autoref{tab:SolcVerifyTable} shows the tool could detect all the vulnerabilities.

\begin{center}
    \begin{table*}
        \caption{SolcVerify results}
                \label{tab:SolcVerifyTable}
                \begin{tabular}{cccl}
                \toprule
                    Attacks & Lines of Code & Execution Time (seconds) & Foud, Not Foud (seconds)\\
                    \midrule
                    Aku & 9 & 4 & Found \\ 
                    Cover & 13  & 5 & Found \\ 
                    BZX & 17  & 9  & Found \\ 
                    Spartan & 25 &  17 & Found \\ 
                    Uranium  &  23 & 9 & Found \\ 
                    XSURGE & 20 & 10 & Found \\  
                    BurgerSwap & 11 & 10  & Found \\ 
                    DirtyDogs &  30 &  14& Found \\
                \bottomrule
        \end{tabular}
    \end{table*}
    
\end{center}

An example of the specifications of SolcVerify is presented by \autoref{lst:SolcVerifyTest}.
The target is DirtyDogs (\autoref{sec:Exploits:DirtyDogs}).
Its program language expresses specifications with annotations, written on top of each function.
Each function requires specifications for the correct tool's behavior.
The annotation \texttt{\_\_verifier\_sum\_uint} makes the sum of the elements contained in the specified mapping.

\begin{lstlisting} [language=Solidity, caption={SolcVerify Specifications}, label={lst:SolcVerifyTest}]
    /// @notice invariant totalClaimednum == effclaimed
    /// @notice invariant __verifier_sum_uint(totalClaimed) <= effclaimed
    contract DirtyDogs is ERC721{
        uint256 totalClaimednum=0;
        uint256 effclaimed=0;
        mapping (address=>uint256)totalClaimed;

        function claimDogs() public {
            uint256 numbersOfTickets = 5 ;
            ///@notice postcondition i==numbersOfTickets
            ///@notice postcondition __verifier_old_uint(effclaimed)==effclaimed+numbersOfTickets
            for(uint256 i = 0; i < numbersOfTickets; i++) {
                uint256 mintIndex = totalSupply_();
                effclaimed++;
                _safeMint(msg.sender, mintIndex);
                //msg.sender.call{ value: 0 }("");
            }
            totalClaimednum=totalClaimednum+(numbersOfTickets);
            totalClaimed[msg.sender] = numbersOfTickets+(totalClaimed[msg.sender]);
        }
    }

\end{lstlisting} 

\subsection*{Slither and Mythril} Slither and Mythil did have any problem during the installation phase and run properly per each attack. 
They could detect all the reentrancy issues, but none of the others.

\section{General Comparison}
%% presenting the outcomes of single analyses, so description of the result 
%% this subsection contains objective data, in the next one I add even my point of view

The first step was the collection of all the data from the analyses with the vulnerable smart contracts as objective. 
In this part, we provide a technical comperison of the tools. 


\subsection{Installation} 
\autoref{tab:Installation} collects the info about the installation details per each tool. 

The running mode involves the way a tool can be run; a tool is classified as "multiple running mode", if its grammar effectively changes between different modes. For example, echidna, it can be run in
test mode and in assertion one, in the first case it requires functions with boolean formulas; in the second case the solidity keyword "assertion" in the code.

The external dependences encompass the external elements, so we do not count the amount of libraries of the same language.
All the tools require Solc, the solidity compiler.
Most of the tools did not have any problems during the installation phase. Celestial and SolcVerify installations had issues.  
The first one needs F* for running and the selection of its right version is
based on the tool and the system environment; consequently, it turned out like a challenge.
On the other hand, the second one involves the usage of Boogie, an intermediate verification language, which requires .NET, (\cite{NET}),
an open source, cross-platform for building many kinds of applications maintained by Microsoft.
The configuration of the correct version of .NET and the other external dependences were based on the computer environment and the tool.

Since Certora operates in Service as a Service (SaaS) mode, it can be considered as the most compatible tool, but the machine requires cannot run offline.
SmartTest could be run in two modes, but it was not necessary, since the running mode without specifications could not provide any consistent results.

\begin{center}
    \begin{table*}
        \caption{Installation and running mode}
        \label{tab:Installation}
        \begin{tabular}{ccccc}
        \toprule
            Tools  &  Running modes & Extenral Dependences & OS \\
            \midrule
            Manticore & 2 & 2 & Linux, OS X\\
            SmartTest & 2 & 3 & Linux, OS X, Windows \\
            Celestial & 1 & 3 & Linux, OS X\\
            Echidna & 2 & 2 & Linux, OS X, Windows\\
            Certora & 1 & -- & -- \\ 
            SolcVerify & 1 & 4  &  Linux, OS X \\
            Mythril  & 1 & 2  &  Linux, OS X \\ 
            Slither & 1 & 2 & Linux, OS X, Windows \\   
        \bottomrule
        \end{tabular}
    \end{table*}
\end{center}

\subsection{Outcomes} 
In this subsection, the outcomes of the tools are compared. 
\autoref{tab:Results} has the name of tools as rows and the real-world exploits as columns.
It shows which tool was able to scan the specified vulnerability involved in the attacks. 
The caption states the symbols used for the evaluation:
\begin{itemize}
    \item \checkmark states that the vulnerability was scanned;
    \item \xmark means the tool was no able to detect it;
    \item -- stands for "discarded", tha attack was not considered for architectural reason of the tool.
\end{itemize}

We can state that the tools with specifications could not detect reentrancy in most of the cases.
SolcVerify is the only one which could provide a result in all cases; moreover, it was the only tool which could detect correctly all the vulnerabilities we provided.

Celestial is the tool with more discarded cases.
The tools without specification could not detect any of the attacks, but the ones involving external calls.

The tools without specifications gave warnings in most cases, which could be used as a hint for scanning the vulnerabilities.
Echidna and Certora had similar behavior; they could not detect the attacks regarding reentrancy.

\begin{center}
    \begin{table*}   
        \caption{Analyses Outcomes per Attack:    
        \checkmark: Found vulenrablity, \xmark: Not found vulnerability, --: Discarded }
        \label{tab:Results}
        \begin{tabular}{ccccccccc}
        \toprule
        Tools  & Aku & Cover & BZX & Spartan & Uranium & XSURGE &  BurgerSwap & DirtyDogs\\
        \midrule
        Manticore & \xmark & \xmark & \checkmark & \checkmark & \xmark & \checkmark & \checkmark & \checkmark\\
        SmartTest & \checkmark &   \xmark & \checkmark  & \xmark &\checkmark  & -- & -- & --  \\
        Celestial & \checkmark & -- & \checkmark & \checkmark & \checkmark & -- & -- & --  \\
        Echidna  & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & -- & -- & -- \\
        Certora & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & -- & -- & -- \\ 
        SolcVerify & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark  & \checkmark \\
        Slither & \xmark &\xmark  &\xmark & \xmark & \xmark & \checkmark & \checkmark & \checkmark \\ 
        Mythril  & \xmark & \xmark & \xmark &\xmark & \xmark & \checkmark & \checkmark & \checkmark\\
        \bottomrule
        \end{tabular}
    \end{table*}
\end{center}

\autoref{tab:Outcomes} provides high level details about the analyses per tool. 
"Constructive output" is referred to the type of output of the tool. 
We have three types:
\begin{enumerate}
    \item list of functions: the sequence of operations to execute for forcing the vulnerability is expressed;
    \item list of unproven tests: the output provides only the name of the failed test;
    \item warnings: the tool displays just warnings.
\end{enumerate}

SolcVerify and Celestial, within the tools allowing custom analyses, are the only ones which do not provide any additional information.
Manticore and Mythril display, when it is possible, the list of functions, but in the case of reentrancy just a warnings of possible risk.

Regarding the speed of the tools, Celestial test times are assumed with zero delay between the generation of the F* code and the running of the F* module.
The time spent for running by command line the two different operations could be avoided by automating the process.
The slowest tools are the ones involving symbolic execution. Mythtil needs a similar amount of time as Manticore and SmartTest, even if it does not allow custom analyses.
Slither is the fastest one, but specifications are not provided per test and no additional information is displayed about the scanned vulnerability.
\begin{center}
    \begin{table*}
        \footnotesize
        \caption{Analyses Outcomes: 
        LoF: List of functions, LoU: List of unproven tests, W: Warnings}
        \label{tab:Outcomes}
        \begin{tabular}{ccccc}
        \toprule
        Tools  & Constructive output &  Avg lines of code for test & Avg time (in seconds) \\
        \midrule
            Manticore & LoF, W  & 4  &  239,5 \\
            SmartTest & LoF, W & 2,5 &  318  \\
            Celestial & LoU & 21  &  4  \\
            Echidna & LoF  & 4  & 20,5 \\
            Certora & LoF   & 34 &  21  \\ 
            SolcVerify & LoU  &  18,5 &  10  \\
            Mythril & LoF, W  & --  &  221  \\ 
            Slither& W & --  &  3,5  \\ 
        \bottomrule
        \end{tabular}
    \end{table*}
\end{center}

Within the tools with specifications, another parameter for comparison is the number of lines of code required for defining the specifications.
The ones which required the least number of lines were the ones which allowed those definitions by using "assertion" or functions returning a boolean statement: SmartTest, Echidna and Manticore.
Howeve, this approach does not have consistent results. Echidna worked better using the other mode.

Echidna and Manticore have similar results since they adopt the same grammar for expressing the specifications.
Those are written in the form of a function which returns a boolean value.

Certora had the most amount of lines. A reason for that is its elasticity for defining specifications in terms of functions.
SolcVerify, with the formal verification tools, had the best behavior in this case, because of its annotation language.
Considering the specification files entirely, Celestial had the longer ones, because the specifications are written in the code.

\chapter{Discussion}
\label{ch:Discussion}

The aim of this chapter is to interpret and explain the obtained results. 
It provides insights on the found threats, having an high level overview. 
Same approach about the tools, which are grouped based on their typology, for focusing their approach and understaning their behavior based on the exploits.

\section{Threats in real-world exploits}

The approach for the choice of the attacks covers their effectiveness in terms of created damage. 
It is estimated based on the stolen amount of money and the possibility to recover the lack of security. 
The targets of those are liquidity pools, automated market makers or NFTs markets. 
Crypto investments are driven by the community, therefore, new solutions would drive the markets.
Consequently, the attackers adapt their target based on that.

A class of attacks involves external calls, particularly reentrancy issues. 
One of the most cited attacks of this type is the DAO attack that happened in 2016. 
Since that, developers should verify problems regarding external calls, but still, nowadays those are still feasible. 

The other exploit deal with problems regarding the sending of possible compensation to users. 
"Uranium", "Spartan" and "Cover" exploits are based on the manipulation of the estimation of rewards. 
The attackers could increase the rewards for withdrawing a greater amount of liquidity from the pool. Those attacks had different methodologies but the same aim. 

The NFTs markets, involved in "Aku" and "DirtyDogs" exploits, were attacked in different ways. 
In the first case, the attack can be classified as denial of service, since the contract was stuck and could not refund the other participants of the bid.
On the other hand, in the second case, the adopted strategy for distributing NFTs with tickets was forced for minting multiple of those. A bad implementation of libraries brought to have that vulnerability. 

It comes clear as the threats had different origins based on the contracts.
Reentrancy is still feasible, even if it is a well-known risk.

The first step in the prevention of attacks is the understanding of the logic of the target contracts. 
Possible attacks can involve the distribution of possible rewards or the process of minting tokens or NFTs with the usage of additional data structures. 



\section{Tools with Specifications}
The tools with specifications allow the users to customize the analysis. 
In this section, those are grouped based on their typology. 

The considered tools cover the following security approach:
\begin{enumerate}
    \item fuzzing;
    \item symbolic execution;
    \item formal verification. 
\end{enumerate} 

\autoref{tab:Strategies} depicts the outcomes of the analysis, grouping the tools based on their typologies. 
The first column is the typology of the tool; the second one is the percentage of detected attacks by the tools of that category. 
The last column has the same role as the previous one, but the considered attacks for estimating the percentage are the ones not involving malicious external calls.

\begin{center}
    \begin{table*}
        \caption{Outcomes based on the typology}
                \label{tab:Strategies}
                \begin{tabular}{cccl}
                \toprule
                    Typology & Execution Time (seconds) & All attacks & No reentrancy\\
                    \midrule
                    Fuzzing & 20,60 & 63\% & 100\% \\ 
                    Formal Verification  & 11,67 & 70\% & 93\% \\ 
                    Symbolic Execution & 272,31  & 44\% & 31\% \\ 
                \bottomrule
        \end{tabular}
    \end{table*}
    
\end{center}


Formal verification is a very powerful security approach, intending to prove or unproven the given specification. 
This perfectly fits with our research goals.
We involved three different tools, in implementing this approach, for our purpose. 

Certora is the only one, within them, which provides a complete list of functions for breaking the rules, rather than just a list of unproven tests. 
On the other hand, SolcVerify could detect vulnerabilities involving external call functions, indeed reentrancy. A powerful aspect of this tool is its possibility to express 
loop invariants, the other ones do not allow it.
Considering the grammar for expressing the specifications, SolcVerify is the one which needs the least amount of lines of code, indeed it involves a notification language.

Certora is the only tool which is not open-source, for our purpose we adopted its free version.
Its specification language is described by its developers' group as "rule-based". 
It differs from the other two tools in this aspect, because the rules are written in a similar way to a function in solidity, consequently, it is more user friendly, and defines more specific cases.
The rule is composed of some function calls and it concludes with an assertion or more, which contains the bool statement of the properties. 
The user is allowed to test a specific case, 
using "require" and the possibility to set up a proper environment. 
A powerful aspect is the possibility to express the assert of the rule in terms of boolean statements and quantifiers as well.

Celestial and SolcVerify needed specifications for all the functions for working properly. 

Echidna is the fuzzer and it had similar results to the formal verification tools. 
It had a bit worse performance in terms of speed and amount of detection. If the exploits with external calls are discarded, this tool could detect all of those. 
It provides the list of functions for forcing the exploit. Since it is based on random inputs, different runnings provided different function inputs.

Symbolic execution is the approach which required the most amount of time. Manticore had a better approach than SmartTest because it allow the detection of malicious external calls. 
Symbolic execution tools try to explore all possible solutions, consequently, they would not have any upper limit of time for exploring all possible cases. 
In some cases, a timeout is fixed by the tools themselves.

In terms of effectiveness and speed, formal verification had the best results.
A strength of the fuzzer and the symbolic execution tools is their outcomes, which display even how the attack is computed. It could help the developer for fixing the bug.

Considering the exploits without malicious external calls, the percentage of detections changes, as shown by \autoref{tab:Strategies}.

Echdina could detect all the vulnerabilities in this case. 
Symbolic execution effectiveness decreased, on the other hand, formal verification increased. 
Those results depict that in the case of smart contracts involving external calls, symbolic execution could provide effective analysis.
The other two approaches are more effective in the case of custom analysis based on specifications, so the detection of exploits involves breaking certain rules. 


\section{Customized and Non-specific Analyses} 
The attackers exploited a specific bug or lack of security in the logic of programs. For scanning those, the involved tools adopted different typologies, allowing custom analysis. 

The specifications allowed the user to express the requirements of the program. The accuracy of the analysis depends on the definition of the specifications. 
Their correctness is fundamental for the effectiveness of the results. 
The definition of those represents a challenge, moreover, each tool has different rules and language for expressing those.
The tools without specification implement automatic detectors. 
Their presentation papers, or documentation, specify which known vulnerabilities can detect. These detect a specified set of vulnerabilities. 
The users should consider this aspect during the analysis. 

Tools of this group, as our results demonstrate, do not have the capability of scanning vulnerabilities involving the logic of the programs. 
However, those obtained consistent results regarding the reentrancy cases. 
An example is Slither, which for every comparison of block time stamps gives a warning. However, developers intentionally consider this case and develop it considering the risks. 

On the other hand, the ones with specifications mostly discarded the malicious external calls. 
SolcVerify was the only tool which could provide the possibility of reentrancy detection. Echidna, as Certora, developers teams specified the tools detect external calls, but only if the code of the attack is provided as well.
However, this approach might be effective for checking a possible attack, so the developers themselves act as malicious actors.

Manticore could bridge this gap by adopting two different running modes, so the user, knowing the limitation of each way of analysis, can combine those for obtaining a valuable result.

The strengths of this group of tools without specifications involve their speed, Slither was the fastest tool, installation and plug-and-play approach. 
Those just require the solidity file of the smart contracts. A user can use those for checking possible reentrancy risks and as the first step of the analysis.

\section{Effective Analysis}
In our work, we took into consideration the selected tools individually. 

We run those per time focusing on the results of each one, and providing a comparison between those.

During an audit or a security report, a tester runs multiple of those for discovering vulnerabilities and bugs. 
A better way to fulfil this goal is using a combination of those. 
The tools without specification have, in our experience, an easier installation and usage, 
due to a reduced amount of external dependencies and writing down specifications is not required.
Those can detect well know vulnerabilities and cover a predefined set of those. 

Some of the tools with specifications we dealt with had some limitations regarding the external calls: just SolcVerify covered this set of vulnerabilities. 
For covering this limitation, a combination of tools would be a solution. 

We should consider a tool without specification, Slither, and one with, Echidna. 
This combination has an effective result in terms of the speed of the analyses and amount of vulnerabilities covered. 
Slither has the role of detecting basic issues and reentrancy, on the other hand, Echidna can be used for the detection of a vulnerable implementation of the logic of the program. 
Since the grammar of this is similar to Manticore, an efficient solution would be to implement the same specification using it, which implements a different logic for scanning. 

Formal verification resulted powerful for scanning possible problems, but SolcVerify and Celestial require to 
write down the specification for all the contracts for obtaining a consistent result. 
Certora has the strength of having implemented libraries which are mostly used in real-world cases (such as OpenZeppelin ones). 
A facilitating aspect of this tool is the possibility to write down the specifications on just the properties we want to check and 
the possibility to code those in terms of function. It allows for the definition of specific preconditions, adding conditions to the environment. 
A strength of this tool is the possibility of using it as SaaS, consequently, the computational effort is demanded from another computer and the installation phase is avoided.

SolcVerify is the tool which obtained the best results in terms of discovering vulnerabilities. 

A possible effective combination can include Echidna and Certora for covering the part of bugs in the logic and possible attacks, 
plus Slither for verifying the absence of possible reentrancy.  

In terms of the effectiveness of discovering vulnerabilities, SolcVerify had the best behaviour, 
but it had issues during the installation phase and writing down the specification per each function increases depending on the size of the smart contract. 
Another drawback is its output, which provides just the list of proven or unproven tests. 
If the specifications are not written correctly, or the tool cannot handle the computation, it states that the tests are not proven, without stating that a problem occurred. 
Consequently, the developer cannot understand if the tool gave a negative result because of a bad implementation or because of the test.
On the other hand, Echidna and Certora specify the list of functions for breaking the specifications. 
Consequently, the outcomes state what is exactly the vulnerability and how to attack it. 
If the developer provides the attacker contract as well, the tools can even state if the attack is effective.
The presented combination is based even on the facility during the writing of the specifications and installation.

