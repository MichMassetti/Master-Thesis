%% LaTeX2e class for student theses
%% sections/evaluation.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute of Information Security and Dependability (KASTEL)
%%
%% Template by
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Adaption by
%% Annika Vielsack
%% vielsack@kit.edu
%%
%% Version 1.0, 2021-07-03

\chapter{Results and Evaluation}
\label{ch:Evaluation}
This chapter presents the results of the analyses. It is addressed to show the obtained data from the tests.

It deals with the individual outcomes per tool, providing details about the running phase and installation.
Eventual problems, during those phases, are described per each tool.

Then, a general comparison gives a high-level view of the obtained results.
This section aims to define which tool had better behaviour in terms of operability, speed and correctness of the analysis. 


\section {Experimental Setup}
The analysis tools were installed and used on a laptop using Ubuntu 20 as the operating system.
The first step for launching the analysis was the selection of smart contracts. 
Their vulnerable sections were considered and the safe parts were discarded, while preserving the foundamental logic. 
The changes applied to the smart contracts have the aim to simplify those and make the analysis faster since the tools have less amount of code to scan. 

The smart contracts were adopted to be tested by all the tools. 
The aim was to obtain results from tools having the same targets.

In Solidity the standard for implementing the token is ERC20. 
Every token inherits the standard interface IERC20.
Their implementation was substituted with arrays, which store the information of the balance of each user.

All smart contracts were adopted to be compiled with Solidity version 7.
Echidna, Celestial and SmarTest had problems with version 8. 
The eighth version solves the problem regarding algebraic issues: 
transactions automatically revert in case of any problem during an algebraic operation. 
In our case, all the smart contracts implemented the "SafeMath", which provides the same functionality check during algebraic operations.

In the case of BZX exploit (\autoref{sec:Exploits:bZX}), which is a vulnerable token,  
the vulnerable function \texttt{transferFrom} is not changed and Array called \texttt{balances} stores the information of the users balances.

The Uranium exploit (\autoref{sec:Exploits:Uranium}) simplified version involves the three main functions of the attacks 
\texttt{deposit}, \texttt{withdraw} and \texttt{emergencyWtihdraw}. 
The struct of the user, which stores the amount of tokens and the reawrds, is not modified.
In this case, the rewards are not sent to any user, but an array stores the amount of the rewards per each users. 

Cover procol exploit(\autore)

\section{Individual Outcomes per Tool}

This section deals with the eventual problems during the installation and running phase , 
moreover, the tools outcomes having as objective a specific attack are collected. 
Per each tool, a table has three parameters; the first one is the number of written lines of code considering the specifications. 
The part of the code which were copied from the target smart contracts were discarded during the counting. 
The second field deals with the execution time of the tool and the last one specifies if the vulnerability was effectively detected.

The code of the exploits was modified to keep the vulnerable aspects, reproducing their logic, but the irrelevant parts were discarded.

We kept the same simplified version of the smart contracts for all the tools.

\paragraph{Manticore} The provided guide on Github (\cite{ManticoreGitHub}) gives a detailed guide for Manticore installation. 
Since it is written in python, we used a virtual environment and we counted around nine libraries for dependences. 
The installation involved basically one command, since it was managed by "pip", python packet manager.
We adopted its default running mode for the attacks involving reentrancy and "Manticore-verifier" running mode for the others. 

The table \autoref*{tab:ManticoreTable} shows the outcames of the analyses per attack; the symbol "--" means that in that case we did not use the 
"Manticore-verifier" running mode, but the default one, which it does not need a specification file.

\begin{center}
\begin{table*}
    \caption{Manticore results}
        \label{tab:ManticoreTable}
        \begin{tabular}{cccc}
        \toprule
            Attacks & Lines of Code & Execution Time (seconds) & Foud, Not Foud\\
            \midrule
            Aku & 6 & 235  & Not Found \\ 
            Cover & 5 & 245 & Not Found \\ 
            BZX  & 4 & 228 & Found \\ 
            Spartan & 3 & 239 & Found \\ 
            Uranium  & 3 & 250 & Not Found \\ 
            XSURGE & -- & 208 & Found \\ 
            BurgerSwap  &  -- & 205 & Found\\ 
            DirtyDogs & -- & 203 & Found \\
        \bottomrule
    \end{tabular}
\end{table*}
\end{center}

\paragraph{SmarTest} SmarTest is built on top of VeriSmart tool, so it can be seen like a plug in of these one. Indeed, we run SmarTest as an option of VeriSmart, 
as the Github guide explains (\cite{SmarTestGitHub}). The tool is built with OCalm, a program language, so we used for the installation "opam", which is a source-based package manager for it.
An important dependency is Z3, its satisfiability modulo theories (SMT) solver. Solc, the compiler for solidity, is required.

Since it has no the dector for reentrancy, as the table \autoref{tab:SmarTestTable} shows, the attacks involving it were discarded. 
We adopted just the "assertion" running mode, for obtating valuable results. 
We fixed a running time threshold of 320 seconds.

\begin{center}
    \begin{table*}
        \caption{SmarTest results}
            \label{tab:SmarTestTable}
            \begin{tabular}{cccl}
            \toprule
                Attacks & Lines of Code & Execution Time (seconds) & Foud, Not Foud\\
                \midrule
                Aku & 3 & 320 & Found \\ 
                Cover & 2 & 310& Not Found\\ 
                BZX  & 4 & 320 & Found\\ 
                Spartan & 2 & 320& Not Found\\ 
                Uranium  & 4 & 320 & Found \\ 
                XSURGE &  -- & -- & Not Found \\  
                BurgerSwap &  -- & -- & Not Found\\ 
                DirtyDogs &  -- & -- & Not Found \\
            \bottomrule
            \end{tabular}
        \end{table*}
\end{center}


\paragraph{Celestial} 
Celestial is a tool which encompasses two main steps: the translation of the target smart contract in F* and then the running of the formal verification engine. 
This is the tool that required the most amount of time for installation and usage. 
It did not cover reentrancy attacks, so those exploits were discarded, moreover, it did not cover the keywords "storage" and "memory", 
so the "Cover Protocol" exploit was discarded as well. \autoref{tab:CelestialTable} shows the tool could detect the other four vulnerabilities.
During the running, we used multiple versions of F* and in some cases (as with the "Uranium" exploit) the conversion was not correct so we adjusted the F* code to let it work. 


\begin{center}
    \begin{table*}
        \caption{Celestial results}
            \label{tab:CelestialTable}
                \begin{tabular}{cccl}
                \toprule
                    Attacks & Lines of Code & Execution Time (seconds) & Foud, Not Foud\\
                    \midrule
                    Aku & 22 & 4 & Found\\ 
                    Cover & --  & -- & Not Found \\ 
                    BZX & 15 & 3 & Found\\ 
                    Spartan & 29 &  5 & Found \\ 
                    Uranium & 18 &  5 & Found \\ 
                    XSURGE &  -- & -- & Not Found \\  
                    BurgerSwap &  -- & -- & Not Found\\ 
                    DirtyDogs &  -- & -- & Not Found \\
                \bottomrule
                \end{tabular}
    \end{table*}
        
\end{center}
\paragraph{Echidna} Echidna is a fuzzer for smart contracts. The tool did have any problem during the installation and worked fluentily. 
It allows to write the properties in form of function or as statement of "assertion". Both solution were applied depeing on the case.
For "BZX" and "Spartan" exploits assertion mode was, the other one for the other cases. 
\autoref{tab:EchidnaTable} states that the tool could detect all the vulnerabilities, 
but the ones involving external calls, since it discards those.


\begin{center}
    \begin{table*}    
        \caption{Echidna results}
        \label{tab:EchidnaTable}
        \begin{tabular}{cccl}
        \toprule
            Attacks & Lines of Code & Execution Time (seconds) & Foud, Not Foud\\
            \midrule
            Aku & 7 & 22 & Found\\ 
            Cover & 5 & 7 & Found\\ 
            BZX & 3 & 33 & Found \\ 
            Spartan & 3 & 17 & Found  \\ 
            Uranium & 3 & 24 & Found \\ 
            XSURGE & -- & -- & Not Found \\  
            BurgerSwap &  -- & -- & Not Found \\ 
            DirtyDogs &  -- & -- & Not Found \\
        \bottomrule
        \end{tabular}
    \end{table*}
\end{center}

\paragraph{Certora} Since Certora is not an open source tool, the provided documentation is limited. 
Their GitHub (\cite{CertoraGitHub}) has a repository dedicated to tutorials for understanding how to write down the properties. 
\autoref{tab:CertoraTable} states that the tool could detect all the vulnerabilities, 
but the ones involving external calls, since it discards those.

\begin{center}
    \begin{table*}
        \caption{Certora results; the time is provided by the sas application}
            \label{tab:CertoraTable}
            \begin{tabular}{cccl}
            \toprule
                Attacks & Lines of Code & Execution Time (seconds) & Foud, Not Foud\\
                \midrule
                Aku & 52 & 14 & Found \\ 
                Cover & 31 & 21 & Found\\ 
                BZX & 25  & 18 & Found\\ 
                Spartan & 20  & 25 & Found\\ 
                Uranium & 42 & 27  & Found\\ 
                XSURGE &  -- & -- & Found\\  
                BurgerSwap &  -- & --& Found\\ 
                DirtyDogs &  -- & -- & Found\\
            \bottomrule
            \end{tabular}
    \end{table*}
\end{center}

\paragraph{SolcVerify} SolcVerify deals with formal verification for smart contracts. 
Its annotation language does not require a great amount of lines of codes. 
For working properly, as Celestial, all the involved functions requires specifications. 
Installation problems occured during the right configuration .NET and the selection of the right version of the external dependences.
Moreover, the tool had issues for finding the correct path of those in the system.

\autoref{tab:SolcVerifyTable} shows the tool could detect all the vulnerabilities.

\begin{center}
    \begin{table*}
        \caption{SolcVerify results}
                \label{tab:SolcVerifyTable}
                \begin{tabular}{cccl}
                \toprule
                    Attacks & Lines of Code & Execution Time (seconds) & Foud, Not Foud (seconds)\\
                    \midrule
                    Aku & 9 & 4 & Found \\ 
                    Cover & 13  & 5 & Found \\ 
                    BZX & 17  & 9  & Found \\ 
                    Spartan & 25 &  17 & Found \\ 
                    Uranium  &  23 & 9 & Found \\ 
                    XSURGE & 20 & 10 & Found \\  
                    BurgerSwap & 11 & 10  & Found \\ 
                    DirtyDogs &  30 &  14& Found \\
                \bottomrule
        \end{tabular}
    \end{table*}
    
\end{center}

\paragraph{Slither and Mythril} Slither and Mythil did have any problem during the installation phase and run properly per each attack. 
They could detect all the reentrancy issues, but none of the others.

\section{General Comparison}
%% presenting the outcomes of single analyses, so description of the result 
%% this subsection contains objective data, in the next one I add even my point of view

The first step was the collection of all the data from the analyses with the vulnerable smart contracts as objective. 
In this part, we provide a technical comperison of the tools. 


\subsection{Installation} 
\autoref{tab:Installation} collects the info about the installation detalais per each tool. 

The running mode involves the way a tool can be run, a tool is classified as "multiple running mode", if its grammar effectivly changes between different modes. For example echidna, it can be run in 
test mode and in assertion one, in the first case it requires functions with boolean formulas, in the second case the solidity keyword "assertion" in the code.

The external dependences encompass the exteranal elements, so we do not count the amount of libraries of the same language. 
All the tools require Solc, the solidity compiler. 
Most of the tools did not have any problem during the installation phase. Celestial and SolcVerify installations had issues.  
The first one needs F* for running and the selection of its right version is
based on the tool and the system environment, consequently, it turned like a challenge. 
On the other hand, the second one involves the usage of Boogie, an intermediate verification language, which requires .NET, (\cite{NET}), 
an open source, cross-platform for building many kinds of applications mantained by Microsoft. 
The configuration of the correct version of .NET and the other external dependences was based on the computer enviroment and the tool. 

Certora operates in Service as a Service (SaaS) mode, so it can be classified as the most compatible tool, but it requires to be connect to internet.
SmartTest can be run in two modes, but it was not necessary, since the running mode without specifications could not provide any consistent results.


\begin{center}
    \begin{table*}
        \caption{Installation and running mode}
        \label{tab:Installation}
        \begin{tabular}{ccccc}
        \toprule
            Tools  &  Running modes & Extenral Dependences & OS \\
            \midrule
            Manticore & 2 & 2 & Linux, OS X\\
            SmartTest & 2 & 3 & Linux, OS X, Windows \\
            Celestial & 1 & 3 & Linux, OS X\\
            Echidna & 2 & 2 & Linux, OS X, Windows\\
            Certora & 1 & -- & -- \\ 
            SolcVerify & 1 & 4  &  Linux, OS X \\
            Mythril  & 1 & 2  &  Linux, OS X \\ 
            Slither & 1 & 2 & Linux, OS X, Windows \\   
        \bottomrule
        \end{tabular}
    \end{table*}
\end{center}

\subsection{Outcomes} 
In this subsection, the outocomes of the tools are compared. 
\autoref{tab:Results} has the name of tools as rows and the real-world exploits as columns.
It shows which tool was able to scan the specified vulnerability involved in the attacks. 
The caption states the symbols used for the evaluation:
\begin{itemize}
    \item \checkmark states that the vulnerability was scanned;
    \item \xmark means the tool was no able to detect it;
    \item -- stands for "discarded", tha attack was not considered for architectural reason of the tool.
\end{itemize}


We can state that the tools with specifications could not detect reentrancy in most of the cases. 
SolcVerify is the only one which could provide a result in all cases, moreover, it was the only tool which could detect correctly all the vulnerabilities we provided.

Celestial is the tool with more discarded cases. 
The tools without specification could not detect any of the attacks, but the ones involving external calls.  

The tools without specifications gave warning in most cases, which can be used as a hint for scanning the vulnerabilities. 
Echidna and Certora had a similar behaviour, they could not detect the attacks regarding reentrancy.


\begin{center}
    \begin{table*}   
        \caption{Analyses Outocomes per Attack:    
        \checkmark: Found vulenrablity, \xmark: Not found vulnerability, --: Discarded }
        \label{tab:Results}
        \begin{tabular}{ccccccccc}
        \toprule
        Tools  & Aku & Cover & BZX & Spartan & Uranium & XSURGE &  BurgerSwap & DirtyDogs\\
        \midrule
        Manticore & \xmark & \xmark & \checkmark & \checkmark & \xmark & \checkmark & \checkmark & \checkmark\\
        SmartTest & \checkmark &   \xmark & \checkmark  & \xmark &\checkmark  & -- & -- & --  \\
        Celestial & \checkmark & -- & \checkmark & \checkmark & \checkmark & -- & -- & --  \\
        Echidna  & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & -- & -- & -- \\
        Certora & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & -- & -- & -- \\ 
        SolcVerify & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark  & \checkmark \\
        Slither & \xmark &\xmark  &\xmark & \xmark & \xmark & \checkmark & \checkmark & \checkmark \\ 
        Mythril  & \xmark & \xmark & \xmark &\xmark & \xmark & \checkmark & \checkmark & \checkmark\\
        \bottomrule
        \end{tabular}
    \end{table*}
\end{center}

\autoref{tab:Outcomes} provides high level detailes about the analyses per tool. 
"Constructive output" is referred to the type of output of the tool. 
We have three types:
\begin{enumerate}
    \item list of functions: the sequence of operations to execute for forcing the vulnerability is expressed;
    \item list of unproved tests: the output provides only the name of the failed test;
    \item warnings: the tool displays just warnings.
\end{enumerate}

SolcVerify and Celestial, within the tools allowing custom analyses, are the only ones which do not provide any additional information. 
Manticore and Mythril display, when it is possible, the list of functions, but in the case of reentrancy just a warnings of possible risk. 

Reagading the speed of the tools, Celestial tets times are assumed with zero delay between the generation of the F* code and the running of the F* module. 
The time spent for running by command line the two different operations could be avoided by automating the process.
The slowest tools are the ones involving symbolic execution. Mythtil needs a similar amount of time as Manticore and SmartTest, even if it does not allow custom analyses. 
Slither is fastest one, but specifications are not provided per test and no additional information are displayed about the scanned vulnerability.
\begin{center}
    \begin{table*}
        \footnotesize
        \caption{Analyses Outcomes: 
        LoF: List of functions, LoU: List of unproved tests, W: Warnings}
        \label{tab:Outcomes}
        \begin{tabular}{ccccc}
        \toprule
        Tools  & Constructive output &  Avg lines of code for test & Avg time (in seconds) \\
        \midrule
            Manticore & LoF, W  & 4  &  239,5 \\
            SmartTest & LoF, W & 2,5 &  318  \\
            Celestial & LoU & 21  &  4  \\
            Echidna & LoF  & 4  & 20,5 \\
            Certora & LoF   & 34 &  21  \\ 
            SolcVerify & LoU  &  18,5 &  10  \\
            Mythril & LoF, W  & --  &  221  \\ 
            Slither& W & --  &  3,5  \\ 
        \bottomrule
        \end{tabular}
    \end{table*}
\end{center}

Within the tools with specifications, another parameter for the comparison is the number of lines of codes required for defining the specifications. 
The ones which required the least number of lines are the ones which allowed those definitions by using "assertion", so SmartTest and Echidna. 
However this approach does not have consistent results, Echidna worked better using the other mode. 

Echidna and Manticore have similar results since they adopt the same grammar for expressing the specifications. 
Those are written in the form of a function which returns a boolean value.

Certora had the most amount of lines. A reason for that is its elasticity for defining specifications in terms of functions. 
SolcVerify, with the formal verification tools, had the best behaviour in this case, because of its annotation language. 
Considering the spefication files entirely, Celestial had the longer ones, because the specifications are written on the code.

\chapter{Discussion}
\label{ch:Discussion}

The aim of this chapter is to interpret and explain the obtained results. 
It provides insights on the found threats, having an high level overview. 
Same approach about the tools, which are grouped based on their startegies, for focusing their approach and understaning their behavior based on the exploits.

\section{Threats in real-world exploits}

The approach for the choice of the attacks covers their effectiveness in terms of created damage. 
It is estimated based on the stolen amount of money and the possibility to recover the lack of security. 
The targets of those are liquidity pools, automated market makers or NFTs markets. 
The decentralised finance investments are driven by the community, therefore, different trends can explode. 
Consequently, the attackers adapt their target based on that.

A class of attacks involves external calls, particularly reentrancy issues. 
One of the most cited attacks of this type is the DAO attack that happened in 2016. 
Since that, developers should verify problems regarding external calls, but still, nowadays those could happen. 

The other exploit deal with problems regarding the sending of possible compensation to users. 
"Uranium", "Spartan" and "Cover" exploits are based on the manipulation of the estimation of rewards. 
The attackers could increase the rewards for withdrawing a greater amount of liquidity from the pool. Those attacks had different methodologies but the same aim. 

The NFTs markets, involved in "Aku" and "DirtyDogs" exploits, were attacked in different ways. 
In the first case, the attack can be classified as denial of service, since the contract was stuck and could not refund the other participants of the bid.
On the other hand, in the second case, the adopted strategy for distributing NFTs with tickets was forced for minting multiple of those. A bad implementation of libraries brought to have that vulnerability. 

It comes clear as the threats had different origins based on the contracts.
Reentrancy is still feasible, even if it is a well-known risk.

The first step in the prevention of attacks is the understanding of the logic of the target contracts. 
Possible attacks can involve the distribution of possible rewards or the process of minting tokens or NFTs with the usage of additional data structures. 



\section{Tools with Specifications}
The tools with specifications allow the users to customize the analysis. 
In this section, those are grouped based on their startegies. 

The considered tools cover the following security approach:
\begin{enumerate}
    \item fuzzing;
    \item symbolic execution;
    \item formal verification. 
\end{enumerate} 

\autoref{tab:Strategies} depicts the outcomes of the analysis, grouping the tools based on their strategies. 
This approach acilitate the comprehension of the behavior of the tecniques in this field.

\begin{center}
    \begin{table*}
        \caption{Outcomes based on strategies}
                \label{tab:Strategies}
                \begin{tabular}{cccl}
                \toprule
                    Strategy & Execution Time (seconds) & Found percentage & No reentrancy\\
                    \midrule
                    Fuzzing & 20,60 & 63\% & 100\% \\ 
                    Formal Verification  & 11,67 & 70\% & 93\% \\ 
                    Symbolic Execution & 272,31  & 44\% & 31\% \\ 
                \bottomrule
        \end{tabular}
    \end{table*}
    
\end{center}

Formal verification is a very powerful security approach, intending to prove or unproved the given specification. 
This perfectly fits with our research goals.
We involved three different tools, in implementing this approach, for our purpose. 

Certora is the only one which provides a complete list of functions for breaking the rules, rather than just a warning. 
On the other hand, SolcVerify could detect the vulnerabilities involving external call functions, indeed reentrancy. A powerful aspect of this tool is its possibility to express 
loop invariants, the other ones do not allow it.
Considering the grammar for expressing the specifications, SolcVerify is the one which needs the least amount of lines of code, indeed it involves a notification language.

Certora is the only tool which is not open-source, for our purpose we adopted its free version.
Its specification language is described by its developers' group as "rule-based". 
It differs from the other two tools under this aspect, because this way gives more elasticity to the user and defines more specific cases.
The rule is composed of some function calls and it concludes with an assertion or more. The user is allowed to test a specific case, 
using "require" and the possibility to set up a proper environment. 

Celestial and SolcVerify needed specifications for all the functions for working properly. 

Echidna is the fuzzer and it had similar results to the formal verification tools. 
It had a bit worse performance in terms of speed and amount of detection. If the exploits with external calls are discarded, this tool could detect all of those. 
It provides the list of functions for forcing the exploit. Since it is based on random inputs, we had cases that run at different times, and the inputs of the function change per time. 

Symbolic execution is the approach which required the most amount of time. Manticore had a better approach than SmartTest because it allow the detection of malicious external calls. 
The tools had a fixed time out for not wasting time, in some cases it expired. 

In terms of effectiveness and speed, formal verification had the best results.
A strength of the fuzzer and the symbolic execution tools is their outcomes, which display even how the attack is computed. It could help the developer for fixing the bug.

Considering the exploits without malicious external calls, the percentage of detections changes, as shown by \autoref{tab:Strategies}.

Echdina could detect all the vulnerabilities in this case. 
Symbolic execution effectiveness decreased, on the other hand, formal verification increased. 
Those results depict that in the case of smart contracts involving external calls, symbolic execution could provide effective analysis.
The other two approaches are more effective in the case of custom analysis based on specifications, so the detection of exploits involves breaking certain rules. 


\section{Customized and Non-specific Analyses} 
The attackers exploited a specific bug or lack of security in the logic of programs. For scanning those, the involved tools adopted different strategies, allowing custom analysis. 

The specifications allowed the user to express the requirements of the program. The accuracy of the analysis depends on the definition of the specifications. 
Their correctness is fundamental for the effectiveness of the results. 
The definition of those represents a challenge, moreover, each tool has different rules and language for expressing those.
The tools without specification implement automatic detectors. 
Their presentation papers, or documentation, specify which known vulnerabilities can detect. These detect a specified set of vulnerabilities. 
The users should consider this aspect during the analysis. 

Tools of this group, as our results demonstrate, do not have the capability of scanning vulnerabilities involving the logic of the programs. 
However, those obtained consistent results regarding the reentrancy cases. 
An example is Slither, which for every comparison of block time stamp gives a warning. However, developers intentionally consider this case and develop it considering the risks. 

On the other hand, the ones with specifications mostly discarded the malicious external calls. 
SolcVerify was the only tool which could provide the possibility of reentrancy detection. Echidna, as Certora, developers teams specified the tools detect external calls, but only if the code of the attack is provided as well.
However, this approach might be effective for checking a possible attack, so the developers themselves act as malicious actors.

Manticore could bridge this gap by adopting two different running modes, so the user, knowing the limitation of each way of analysis, can combine those for obtaining a valuable result.

The strengths of this group of tools without specifications involve their speed, Slither was the fastest tool, installation and plug and play approach. 
Those just require the solidity file of the smart contracts. A user can use those for checking possible reentrancy risks and as the first step of the analysis.

\section{Effective Analysis}
In our work, we took into consideration the selected tools individually. 

We run those per time focusing on the results of each one, and providing a comparison between those.

During an audit or a security report, a tester runs multiple of those for discovering vulnerabilities and bugs. 
A better way to fulfil this goal is using a combination of those. 
The tools without specification have, in our experience, an easier installation and usage, 
due to a reduced amount of external dependences and writing down specification is not required.
Those can detect well know vulnerabilities and cover a predefined set of those. 

Some of the tools with specifications we dealt with had some limitations regarding the external calls: just SolcVerify covered this set of vulnerabilities. 
For covering this limitation, a combination of tools would be a solution. 

We should consider a tool without specification, Slither, and one with, Echidna. 
This combination has an effective result in terms of the speed of the analyses and amount of vulnerabilities covered. 
Slither has the role of detecting basic issues and reentrancy, on the other hand, Echidna can be used for the detection of a vulnerable implementation of the logic of the program. 
Since the grammar of this is similar to Manticore, an efficient solution would be to implement the same specification using it, which implements a different logic for scanning. 

Formal verification resulted powerful for scanning possible problems, but SolcVerify and Celestial require to 
write down the specification for all the contracts for obtaining a consistent result. 
Certora has the strength of having implemented libraries which are mostly used in real-world cases (as OpenZeppelin ones,Aeve protocol). 
A facilitating aspect of this tool is the possibility to write down the specifications on just the properties we want to check and 
the possibility to code those in terms of function. It allows for the definition of specific preconditions, adding conditions to the environment. 
Another strenght of this tool is the possibility of using it as SaaS, consequently, the computational effort is demanded to another computer.

SolcVerify is the tool which obtained the best results in term of discovering vulnerabilities. One of the drawback we found is the Ã¨

A possible effective combination can include Echidna and Certora for covering the part of bugs in the logic and possible attacks, 
plus Slither for verifying the absence of possible reentrancy. 

In term of effectivenss of discovering vulnerabilities, SolcVerify had the best behaviour, 
but it had issues during the installation phase and writing down the specification per each function increases depending the size of the smart contract.
The presented combination is based on the facility during the writing the specifications and installation.


